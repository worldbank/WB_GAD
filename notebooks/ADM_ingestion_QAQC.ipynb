{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA/QC for delivered administrative boundaries\n",
    "The World Bank receives regular deliveries of official administrative boundaries. This script will process and evaluate these boundaries in several ways:\n",
    "\n",
    "- Combine the two ID columns into a single, primary key  \n",
    "  a. Check to ensure no duplicates in this new, primary key\n",
    "- Combine ADM0 file with disputed areas shapefile \n",
    "- Create ocean mask \n",
    "- Within higher heirarchical level, evaluate name duplication\n",
    "- Perform topological checks  \n",
    "  a. Ensure no overlaps  \n",
    "  b. Ensure no gaps  \n",
    "  c. Ensure all admin1 and admin2 shapes are fully contained within their heirarchical parents\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "from shapely.geometry import Point, Polygon, box, shape\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from wb_gad_helper import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_folder = r'C:\\WBG\\Work\\data\\ADMIN\\NEW_WB_BOUNDS'\n",
    "out_folder = r'C:\\WBG\\Work\\data\\ADMIN\\QAQC'\n",
    "better_formats_folder = r'C:\\WBG\\Work\\data\\ADMIN\\BETTER_FORMATS'\n",
    "if not os.path.exists(better_formats_folder):\n",
    "    os.makedirs(better_formats_folder)\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "\n",
    "admin0_file = os.path.join(admin_folder, 'WB_GAD_ADM0.shp')\n",
    "adm0_disputes_file = os.path.join(admin_folder, 'WB_GAD_NDLSA.shp')\n",
    "admin1_file = os.path.join(admin_folder, 'WB_GAD_ADM1.shp')\n",
    "admin2_file = os.path.join(admin_folder, 'WB_GAD_ADM2.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>group</th>\n",
       "      <th>ISO_A3</th>\n",
       "      <th>CONTINENT</th>\n",
       "      <th>WB_REGION</th>\n",
       "      <th>WB_INCOME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABW</td>\n",
       "      <td>North America</td>\n",
       "      <td>LCN</td>\n",
       "      <td>HIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>SAS</td>\n",
       "      <td>LIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGO</td>\n",
       "      <td>Africa</td>\n",
       "      <td>AFE,SSF</td>\n",
       "      <td>LMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AIA</td>\n",
       "      <td>North America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALA</td>\n",
       "      <td>Europe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "group ISO_A3      CONTINENT WB_REGION WB_INCOME\n",
       "0        ABW  North America       LCN       HIC\n",
       "1        AFG           Asia       SAS       LIC\n",
       "2        AGO         Africa   AFE,SSF       LMC\n",
       "3        AIA  North America       NaN       NaN\n",
       "4        ALA         Europe       NaN       NaN"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add World Bank classifications to ADM0\n",
    "wb_classes = get_wb_classifications_strict(grouping_version=\"37.0\", region_version=\"2.0\", income_version=\"2.0\",)\n",
    "wb_classes.rename({'ISO3':'ISO_A3', 'REGION':'WB_REGION', 'INCOME':'WB_INCOME'}, inplace=True, axis=1)\n",
    "wb_classes['CONTINENT'] = wb_classes['CONTINENT'].fillna(\"001\")\n",
    "continent_names_map = {\n",
    "    '002':'Africa',\n",
    "    '005':'South America',\n",
    "    '009':'Oceania',\n",
    "    '142':'Asia',\n",
    "    '150':'Europe',\n",
    "    '001':'North America'\n",
    "}\n",
    "wb_classes['CONTINENT'] = wb_classes['CONTINENT'].map(continent_names_map)\n",
    "wb_classes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm0 = gpd.read_file(admin0_file)\n",
    "adm0_disputes = gpd.read_file(adm0_disputes_file)\n",
    "adm1 = gpd.read_file(admin1_file)\n",
    "adm2 = gpd.read_file(admin2_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm0.drop(\"WB_REGION\", axis=1, inplace=True)\n",
    "adm0 = pd.merge(adm0, wb_classes, on='ISO_A3')\n",
    "#adm0.to_file(os.path.join(out_folder, \"adm0_merged_classes.gpkg\"), driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine ID columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_adm1 = merge_id_columns(adm1, [['P_CODE_1', 'P_CODE_1_t'], ['ADM1CD', 'ADM1CD_t']])\n",
    "merged_adm2 = merge_id_columns(adm2, [['P_CODE_1', 'P_CODE_1_t'], ['P_CODE_2', 'P_CODE_2_t'], ['ADM1CD', 'ADM1CD_t'], ['ADM2CD', 'ADM2CD_t']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADM1CD_c duplicates: 0\n",
      "ADM2CD_c duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in ADM1\n",
    "test_col = 'ADM1CD_c'\n",
    "check_duplicates(merged_adm1, test_col, os.path.join(out_folder, f'adm1_duplicates_{test_col}.gpkg'))\n",
    "\n",
    "# Check for duplicates in ADM2\n",
    "test_col = 'ADM2CD_c'\n",
    "check_duplicates(merged_adm2, test_col, os.path.join(out_folder, f'adm2_duplicates_{test_col}.gpkg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine ADM0 with disputed territories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in adm0_disputes.columns:\n",
    "    if not col in adm0.columns:\n",
    "        adm0_disputes.drop(columns=[col], inplace=True)\n",
    "adm0_disputes.head()\n",
    "adm0_complete = pd.concat([adm0, adm0_disputes], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a global ocean mask from the admin divisions\n",
    "ocean_polygon = box(-180, -90, 180, 90)  # Global bounding box for ocean\n",
    "#clip ocean polygon to adm0 boundaries\n",
    "ocean_polygon = ocean_polygon.difference(adm0_complete.union_all())  # Remove land areas\n",
    "ocean_mask = gpd.GeoDataFrame([[1, ocean_polygon]], columns=['id', 'geometry'], crs=4326)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate name duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate duplicate names in adm1 and adm2\n",
    "evaluate_duplicate_names(merged_adm1, 'NAM_1', 'ISO_A3', os.path.join(out_folder, \"ADM1_name_duplicates.log\"))\n",
    "# Evaluate duplicate names in adm1 and adm2\n",
    "evaluate_duplicate_names(merged_adm2, 'NAM_2', 'ADM1CD_c', os.path.join(out_folder, \"ADM2_name_duplicates.log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsj = gpd.sjoin(adm0, adm0, how=\"inner\", predicate=\"overlaps\", lsuffix=\"left\", rsuffix=\"right\")\\nsj = sj[sj.index != sj.index_right]\\n\\nsj[\\'intersection_geom\\'] = sj[\\'geometry_left\\'].intersection(sj[\\'geometry_right\\'])\\nsj[\\'intersection_area\\'] = sj[\\'intersection_geom\\'].area\\nsj\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sjoin to identify overlapping polygons\n",
    "'''\n",
    "sj = gpd.sjoin(adm0, adm0, how=\"inner\", predicate=\"overlaps\", lsuffix=\"left\", rsuffix=\"right\")\n",
    "sj = sj[sj.index != sj.index_right]\n",
    "\n",
    "sj['intersection_geom'] = sj['geometry_left'].intersection(sj['geometry_right'])\n",
    "sj['intersection_area'] = sj['intersection_geom'].area\n",
    "sj\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate datasets into primary and secondary tables\n",
    "\n",
    "The delivered files contain several columns that are temporary or reference external sources. This section will separate the superfluous columns into a secondary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_out_folder = os.path.join(better_formats_folder, 'simplified_output')\n",
    "if not os.path.exists(simplified_out_folder):\n",
    "    os.makedirs(simplified_out_folder)\n",
    "adm1_primary = 'ADM1CD_c'\n",
    "adm1_simple_cols = ['ISO_A3','ISO_A2','WB_A3','WB_REGION','WB_STATUS','NAM_0','NAM_1','ADM1CD_c','GEOM_SRCE', 'geometry']\n",
    "adm1_bad_cols = [adm1_primary] + [x for x in merged_adm1.columns if x not in adm1_simple_cols]\n",
    "\n",
    "adm2_primary = 'ADM2CD_c'\n",
    "adm2_simple_cols = adm1_simple_cols + ['NAM_2','ADM2CD_c']\n",
    "adm2_bad_cols = [adm2_primary] + [x for x in merged_adm2.columns if x not in adm2_simple_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out final versions in several data formats\n",
    "\n",
    "write out:\n",
    "- adm0 base\n",
    "- adm0 NDLSA\n",
    "- adm0 complete\n",
    "- adm1 simple\n",
    "- adm1 supplemntal columns\n",
    "- adm2 simple \n",
    "- adm2 supplemental columns\n",
    "\n",
    "in these formats\n",
    "- geopackage\n",
    "- geojson\n",
    "- shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adm1_bad_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Write geojson to file\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     gdf\u001b[38;5;241m.\u001b[39mto_file(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(final_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeojson\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.geojson\u001b[39m\u001b[38;5;124m\"\u001b[39m), driver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeoJSON\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(merged_adm1\u001b[38;5;241m.\u001b[39mloc[:, \u001b[43madm1_bad_cols\u001b[49m])\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(final_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWB_GAD_adm1_additional_columns.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(merged_adm2\u001b[38;5;241m.\u001b[39mloc[:, adm2_bad_cols])\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(final_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWB_GAD_adm2_additional_columns.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'adm1_bad_cols' is not defined"
     ]
    }
   ],
   "source": [
    "final_folder = os.path.join(admin_folder, 'FOR_PUBLICATION')\n",
    "if not os.path.exists(final_folder):\n",
    "    os.makedirs(final_folder)\n",
    "for file_format in ['gpkg', 'shp', 'geojson']:\n",
    "    temp_folder = os.path.join(final_folder, file_format)\n",
    "    if not os.path.exists(temp_folder):\n",
    "        os.makedirs(temp_folder)\n",
    "\n",
    "for file_def in [\n",
    "    (ocean_mask, 'WB_GAD_ocean_mask'),\n",
    "    (adm0_complete, 'WB_GAD_ADM0_complete'),\n",
    "    (adm0, 'WB_GAD_ADM0'),\n",
    "    (adm0_disputes, 'WB_GAD_ADM0_NDLSA'),\n",
    "    (merged_adm1.loc[:, adm1_simple_cols], 'WB_GAD_ADM1'),\n",
    "    (merged_adm2.loc[:, adm2_simple_cols], 'WB_GAD_ADM2'),    \n",
    "    ]:\n",
    "    gdf, filename = file_def\n",
    "    # write geopackage to file\n",
    "    gdf.to_file(os.path.join(final_folder, \"gpkg\", f\"{filename}.gpkg\"), driver='GPKG')\n",
    "    # write shapefile to file\n",
    "    gdf.to_file(os.path.join(final_folder, \"shp\", f\"{filename}.shp\"), driver='ESRI Shapefile')\n",
    "    # Write geojson to file\n",
    "    gdf.to_file(os.path.join(final_folder, \"geojson\", f\"{filename}.geojson\"), driver='GeoJSON')\n",
    "\n",
    "pd.DataFrame(merged_adm1.loc[:, adm1_bad_cols]).to_csv(os.path.join(final_folder, 'WB_GAD_adm1_additional_columns.csv'), index=False)\n",
    "pd.DataFrame(merged_adm2.loc[:, adm2_bad_cols]).to_csv(os.path.join(final_folder, 'WB_GAD_adm2_additional_columns.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
